{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intent Classifier Module\n",
    "Train on the predefined question set with Intent, and classify the intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mgmgk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mgmgk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# checking if nltk data is available, if not download it\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intent classifier class\n",
    "class IntentClassifier:\n",
    "    \"\"\"\n",
    "    A class for classifying the intent of user questions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_type='logistic', vectorizer_type='tfidf'):\n",
    "        \"\"\"\n",
    "        Initialize the IntentClassifier with the specified model type.\n",
    "        \n",
    "        Args:\n",
    "            model_type (str): The type of model to use for classification.\n",
    "                Options: 'naive_bayes', 'logistic', 'random_forest', 'svm'\n",
    "            vectorizer_type (str): The type of vectorizer to use.\n",
    "                Options: 'tfidf', 'count'\n",
    "        \"\"\"\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.model_type = model_type\n",
    "        self.vectorizer_type = vectorizer_type\n",
    "        self.pipeline = None\n",
    "        self.intent_labels = None\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocess the text by removing punctuation, converting to lowercase,\n",
    "        removing stopwords, and lemmatizing.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to preprocess.\n",
    "            \n",
    "        Returns:\n",
    "            str: The preprocessed text.\n",
    "        \"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Simple tokenization by splitting on whitespace\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def create_model(self):\n",
    "        \"\"\"\n",
    "        Create a pipeline with vectorizer and the selected classifier.\n",
    "        \n",
    "        Returns:\n",
    "            sklearn.pipeline.Pipeline: The created pipeline.\n",
    "        \"\"\"\n",
    "        # Select vectorizer\n",
    "        if self.vectorizer_type == 'tfidf':\n",
    "            vectorizer = TfidfVectorizer(\n",
    "                preprocessor=self.preprocess_text,\n",
    "                ngram_range=(1, 2),  # Use both unigrams and bigrams\n",
    "                min_df=2,  # Minimum document frequency\n",
    "                max_df=0.9  # Maximum document frequency\n",
    "            )\n",
    "        else:  # count vectorizer\n",
    "            vectorizer = CountVectorizer(\n",
    "                preprocessor=self.preprocess_text,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.9\n",
    "            )\n",
    "        \n",
    "        # Select classifier\n",
    "        if self.model_type == 'naive_bayes':\n",
    "            classifier = MultinomialNB(alpha=0.1)\n",
    "        elif self.model_type == 'logistic':\n",
    "            classifier = LogisticRegression(\n",
    "                C=10.0,\n",
    "                max_iter=1000,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "        elif self.model_type == 'random_forest':\n",
    "            classifier = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=None,\n",
    "                min_samples_split=2,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "        elif self.model_type == 'svm':\n",
    "            classifier = SVC(\n",
    "                kernel='linear',\n",
    "                C=1.0,\n",
    "                probability=True,\n",
    "                class_weight='balanced',\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "        \n",
    "        return Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "    \n",
    "    def train(self, data_path, test_size=0.2, random_state=42, optimize=False):\n",
    "        \"\"\"\n",
    "        Train the intent classification model using the provided data.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the CSV file containing the training data.\n",
    "            test_size (float): Proportion of the data to use for testing.\n",
    "            random_state (int): Random seed for reproducibility.\n",
    "            optimize (bool): Whether to perform hyperparameter optimization.\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary containing the evaluation metrics.\n",
    "        \"\"\"\n",
    "        # Load the data\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        # Clean column names (remove leading/trailing whitespace)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        # Clean data (remove leading/trailing whitespace)\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'object':\n",
    "                df[col] = df[col].str.strip()\n",
    "        \n",
    "        # Store the intent labels\n",
    "        self.intent_labels = sorted(df['Intent'].unique())\n",
    "        \n",
    "        # Print dataset statistics\n",
    "        print(f\"Dataset size: {len(df)} questions\")\n",
    "        print(f\"Intent categories: {len(self.intent_labels)}\")\n",
    "        print(\"Intent distribution:\")\n",
    "        intent_counts = df['Intent'].value_counts()\n",
    "        for intent, count in intent_counts.items():\n",
    "            print(f\"  {intent}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        # Use stratification only if all classes have at least 2 samples\n",
    "        min_samples_per_class = df['Intent'].value_counts().min()\n",
    "        if min_samples_per_class >= 2:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                df['Question'], df['Intent'], test_size=test_size, \n",
    "                random_state=random_state, stratify=df['Intent']\n",
    "            )\n",
    "        else:\n",
    "            print(\"Warning: Some classes have too few samples for stratified splitting. Using regular train-test split.\")\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                df['Question'], df['Intent'], test_size=test_size, \n",
    "                random_state=random_state\n",
    "            )\n",
    "        \n",
    "        # Create the pipeline\n",
    "        self.pipeline = self.create_model()\n",
    "        \n",
    "        # Perform hyperparameter optimization if requested\n",
    "        if optimize:\n",
    "            print(\"\\nPerforming hyperparameter optimization...\")\n",
    "            if self.model_type == 'naive_bayes':\n",
    "                param_grid = {\n",
    "                    'classifier__alpha': [0.01, 0.1, 0.5, 1.0, 2.0]\n",
    "                }\n",
    "            elif self.model_type == 'logistic':\n",
    "                param_grid = {\n",
    "                    'classifier__C': [0.1, 1.0, 10.0, 100.0],\n",
    "                    'classifier__solver': ['liblinear', 'saga']\n",
    "                }\n",
    "            elif self.model_type == 'random_forest':\n",
    "                param_grid = {\n",
    "                    'classifier__n_estimators': [50, 100, 200],\n",
    "                    'classifier__max_depth': [None, 10, 20, 30]\n",
    "                }\n",
    "            elif self.model_type == 'svm':\n",
    "                param_grid = {\n",
    "                    'classifier__C': [0.1, 1.0, 10.0],\n",
    "                    'classifier__kernel': ['linear', 'rbf']\n",
    "                }\n",
    "            \n",
    "            grid_search = GridSearchCV(\n",
    "                self.pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n",
    "            )\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            \n",
    "            print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "            self.pipeline = grid_search.best_estimator_\n",
    "        else:\n",
    "            # Train the pipeline\n",
    "            self.pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        y_pred = self.pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'classification_report': report,\n",
    "            'test_data': (X_test, y_test, y_pred)\n",
    "        }\n",
    "    \n",
    "    def predict(self, question):\n",
    "        \"\"\"\n",
    "        Predict the intent of a given question.\n",
    "        \n",
    "        Args:\n",
    "            question (str): The question to classify.\n",
    "            \n",
    "        Returns:\n",
    "            str: The predicted intent.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        return self.pipeline.predict([question])[0]\n",
    "    \n",
    "    def predict_proba(self, question):\n",
    "        \"\"\"\n",
    "        Predict the probability of each intent for a given question.\n",
    "        \n",
    "        Args:\n",
    "            question (str): The question to classify.\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary mapping intent labels to their probabilities.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        probabilities = self.pipeline.predict_proba([question])[0]\n",
    "        return {intent: prob for intent, prob in zip(self.pipeline.classes_, probabilities)}\n",
    "    \n",
    "    def save_model(self, file_path):\n",
    "        \"\"\"\n",
    "        Save the trained model to a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to save the model.\n",
    "        \"\"\"\n",
    "        if self.pipeline is None:\n",
    "            raise ValueError(\"Model not trained. Call train() first.\")\n",
    "        \n",
    "        import joblib\n",
    "        joblib.dump(self.pipeline, file_path)\n",
    "        print(f\"Model saved to {file_path}\")\n",
    "    \n",
    "    def load_model(self, file_path):\n",
    "        \"\"\"\n",
    "        Load a trained model from a file.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the saved model.\n",
    "        \"\"\"\n",
    "        import joblib\n",
    "        self.pipeline = joblib.load(file_path)\n",
    "        print(f\"Model loaded from {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation function\n",
    "def evaluate_models(data_path, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate different models and vectorizers to find the best combination.\n",
    "    \n",
    "    Args:\n",
    "        data_path (str): Path to the CSV file containing the training data.\n",
    "        test_size (float): Proportion of the data to use for testing.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: The best model type and vectorizer type.\n",
    "    \"\"\"\n",
    "    models = ['naive_bayes', 'logistic', 'random_forest', 'svm']\n",
    "    vectorizers = ['tfidf', 'count']\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"Evaluating different model and vectorizer combinations:\")\n",
    "    for model_type in models:\n",
    "        for vectorizer_type in vectorizers:\n",
    "            print(f\"\\nTesting {model_type} with {vectorizer_type} vectorizer...\")\n",
    "            classifier = IntentClassifier(model_type=model_type, vectorizer_type=vectorizer_type)\n",
    "            metrics = classifier.train(data_path, test_size=test_size, random_state=random_state)\n",
    "            \n",
    "            key = f\"{model_type}_{vectorizer_type}\"\n",
    "            results[key] = metrics['accuracy']\n",
    "            print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    # Find the best combination\n",
    "    best_key = max(results, key=results.get)\n",
    "    best_model, best_vectorizer = best_key.split('_')\n",
    "    \n",
    "    print(\"\\nResults summary:\")\n",
    "    for key, accuracy in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{key}: {accuracy:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBest combination: {best_model} with {best_vectorizer} vectorizer (Accuracy: {results[best_key]:.4f})\")\n",
    "    \n",
    "    return best_model, best_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Dataset size: 500 questions\n",
      "Intent categories: 7\n",
      "Intent distribution:\n",
      "  password: 75 (15.0%)\n",
      "  network: 75 (15.0%)\n",
      "  application: 75 (15.0%)\n",
      "  email: 75 (15.0%)\n",
      "  internet: 75 (15.0%)\n",
      "  hardware: 75 (15.0%)\n",
      "  device: 50 (10.0%)\n",
      "\n",
      "Model Accuracy: 1.0000\n",
      "\n",
      "Classification Report:\n",
      "Intent: application\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1-score: 1.0000\n",
      "  Support: 15.0\n",
      "Intent: device\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1-score: 1.0000\n",
      "  Support: 10.0\n",
      "Intent: email\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1-score: 1.0000\n",
      "  Support: 15.0\n",
      "Intent: hardware\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1-score: 1.0000\n",
      "  Support: 15.0\n",
      "Intent: internet\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1-score: 1.0000\n",
      "  Support: 15.0\n",
      "Intent: network\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1-score: 1.0000\n",
      "  Support: 15.0\n",
      "Intent: password\n",
      "  Precision: 1.0000\n",
      "  Recall: 1.0000\n",
      "  F1-score: 1.0000\n",
      "  Support: 15.0\n",
      "\n",
      "Testing with example questions:\n",
      "\n",
      "Question: I need to change my password for Windows\n",
      "Predicted Intent: password\n",
      "Top 3 Probabilities:\n",
      "  password: 0.9861\n",
      "  application: 0.0026\n",
      "  hardware: 0.0025\n",
      "\n",
      "Question: My computer won't connect to the internet\n",
      "Predicted Intent: internet\n",
      "Top 3 Probabilities:\n",
      "  internet: 0.4482\n",
      "  network: 0.1744\n",
      "  hardware: 0.1617\n",
      "\n",
      "Question: How do I install a new software?\n",
      "Predicted Intent: hardware\n",
      "Top 3 Probabilities:\n",
      "  hardware: 0.5802\n",
      "  application: 0.1965\n",
      "  internet: 0.0522\n",
      "\n",
      "Question: I can't access my email account\n",
      "Predicted Intent: email\n",
      "Top 3 Probabilities:\n",
      "  email: 0.9419\n",
      "  network: 0.0150\n",
      "  application: 0.0113\n",
      "\n",
      "Question: My printer is not working properly\n",
      "Predicted Intent: hardware\n",
      "Top 3 Probabilities:\n",
      "  hardware: 0.2935\n",
      "  network: 0.2097\n",
      "  application: 0.1688\n",
      "\n",
      "Question: How do I connect to the company VPN?\n",
      "Predicted Intent: network\n",
      "Top 3 Probabilities:\n",
      "  network: 0.7687\n",
      "  device: 0.0565\n",
      "  internet: 0.0522\n",
      "\n",
      "Question: My laptop battery is draining too quickly\n",
      "Predicted Intent: device\n",
      "Top 3 Probabilities:\n",
      "  device: 0.8644\n",
      "  application: 0.0253\n",
      "  hardware: 0.0253\n"
     ]
    }
   ],
   "source": [
    "# data_path = 'it_faqs_and_logs.csv'         # short set\n",
    "# data_path = 'it_support_questions.csv'      # 100 samples\n",
    "data_path = 'it_support_questions_500.csv'  # 500 samples\n",
    "     \n",
    "classifier = IntentClassifier(model_type='logistic', vectorizer_type='tfidf')\n",
    "    \n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "metrics = classifier.train(data_path, optimize=False)  # Disable optimization for small datasets\n",
    "    \n",
    "# Print evaluation metrics\n",
    "print(f\"\\nModel Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "for intent, values in metrics['classification_report'].items():\n",
    "    if intent not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "        print(f\"Intent: {intent}\")\n",
    "        print(f\"  Precision: {values['precision']:.4f}\")\n",
    "        print(f\"  Recall: {values['recall']:.4f}\")\n",
    "        print(f\"  F1-score: {values['f1-score']:.4f}\")\n",
    "        print(f\"  Support: {values['support']}\")\n",
    "    \n",
    "# Test with some example questions\n",
    "print(\"\\nTesting with example questions:\")\n",
    "test_questions = [\n",
    "    \"I need to change my password for Windows\",\n",
    "    \"My computer won't connect to the internet\",\n",
    "    \"How do I install a new software?\",\n",
    "    \"I can't access my email account\",\n",
    "    \"My printer is not working properly\",\n",
    "    \"How do I connect to the company VPN?\",\n",
    "    \"My laptop battery is draining too quickly\"\n",
    "]\n",
    "    \n",
    "for question in test_questions:\n",
    "    intent = classifier.predict(question)\n",
    "    probabilities = classifier.predict_proba(question)\n",
    "    top_intents = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "        \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Predicted Intent: {intent}\")\n",
    "    print(\"Top 3 Probabilities:\")\n",
    "    for intent_label, prob in top_intents:\n",
    "        print(f\"  {intent_label}: {prob:.4f}\")\n",
    "    \n",
    "# Save the model (uncomment to save)\n",
    "# classifier.save_model('intent_classifier_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted Intent: internet\n",
      "Top Probabilities:\n",
      "  internet: 0.7240\n",
      "  application: 0.1035\n",
      "  hardware: 0.0394\n"
     ]
    }
   ],
   "source": [
    "# Accept user input\n",
    "user_question = input(\"Enter your question: \")\n",
    "\n",
    "# Classify the intent\n",
    "predicted_intent = classifier.predict(user_question)\n",
    "predicted_probabilities = classifier.predict_proba(user_question)\n",
    "\n",
    "# Display the results\n",
    "print(f\"\\nPredicted Intent: {predicted_intent}\")\n",
    "print(\"Top Probabilities:\")\n",
    "for intent_label, prob in sorted(predicted_probabilities.items(), key=lambda x: x[1], reverse=True)[:3]:\n",
    "    print(f\"  {intent_label}: {prob:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
